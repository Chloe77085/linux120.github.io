<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-ai-developer-workflow" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">AI 开发者工作流程 | Linux 1.2.0</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://linux120.github.io/linux120.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://linux120.github.io/linux120.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://linux120.github.io/linux120.github.io/docs/ai-developer-workflow"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AI 开发者工作流程 | Linux 1.2.0"><meta data-rh="true" name="description" content="魔方派3 AI 开发者工作流程"><meta data-rh="true" property="og:description" content="魔方派3 AI 开发者工作流程"><link data-rh="true" rel="icon" href="/linux120.github.io/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://linux120.github.io/linux120.github.io/docs/ai-developer-workflow"><link data-rh="true" rel="alternate" href="https://linux120.github.io/linux120.github.io/docs/ai-developer-workflow" hreflang="en"><link data-rh="true" rel="alternate" href="https://linux120.github.io/linux120.github.io/docs/ai-developer-workflow" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AI 开发者工作流程","item":"https://linux120.github.io/linux120.github.io/docs/ai-developer-workflow"}]}</script><link rel="alternate" type="application/rss+xml" href="/linux120.github.io/blog/rss.xml" title="Linux 1.2.0 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/linux120.github.io/blog/atom.xml" title="Linux 1.2.0 Atom Feed"><link rel="stylesheet" href="/linux120.github.io/assets/css/styles.4fff4710.css">
<script src="/linux120.github.io/assets/js/runtime~main.b2f288e5.js" defer="defer"></script>
<script src="/linux120.github.io/assets/js/main.eb76ef4f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/linux120.github.io/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/linux120.github.io/"><div class="navbar__logo"><img src="/linux120.github.io/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/linux120.github.io/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/linux120.github.io/docs/peripheral-compatibility-list">Tutorial</a><a class="navbar__item navbar__link" href="/linux120.github.io/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/peripheral-compatibility-list">外设兼容列表</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/revision-history">文档更新说明</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/linux120.github.io/docs/quick-start/">快速入门</a><button aria-label="Expand sidebar category &#x27;快速入门&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/peripherals-and-interfaces">外设与接口</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/linux-kernel">Linux 内核</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/qualcomm-ai-hub">Qualcomm AI Hub</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/linux120.github.io/docs/ai-developer-workflow">AI 开发者工作流程</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/qualcomm-im-sdk">Qualcomm IM SDK</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/lvgl-user-guide">LVGL 使用指南</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/qt5-user-guide">QT5 使用指南</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/tools-and-libraries-porting-guide">工具&amp;库移植指南</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/ros-user-guide">ROS 使用指南</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/yocto-project-user-guide">Yocto 工程编译指南</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/user-guide-to-rubikpi-config">rubikpi_config 使用指南</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/gitHub-operation-guide">GitHub操作指南</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/linux120.github.io/docs/set-up-development-environment">开发环境搭建</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/linux120.github.io/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">AI 开发者工作流程</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>AI 开发者工作流程</h1></header>
<p>Qualcomm Linux 中的 AI/ML 开发人员工作流程主要分为两个步骤：
步骤 1：</p>
<p>编译并优化模型</p>
<ul>
<li>编译并优化来自第三方 AI 框架的模型，以便在 RUBIK Pi 3 上高效运行。例如，可以将 TensorFlow 模型导出为 TFLite 模型。</li>
<li>或者，使用硬件特定的定制来量化、细调性能和精确度。</li>
</ul>
<p>步骤 2：</p>
<p>编译应用程序，使用优化后的模型在设备上运行推理</p>
<ul>
<li>将 AI 模型集成到用例 pipeline 中。</li>
<li>或者使用安卓的 native sdk 调用 Qualcomm AI SDKs。</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="概述">概述<a href="#概述" class="hash-link" aria-label="Direct link to 概述" title="Direct link to 概述">​</a></h2>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-156-eae5d8c028549e1c0bd52fca6c44fef7.jpg" width="1000" height="605" class="img_ev3q"></p>
<p>开发人员可以从 ONNX、PyTorch、TensorFlow 或 TFLite 引入模型，并使用 Qualcomm AI SDK 在 Qualcomm AI 硬件 - HTP (NPU)、GPU、CPU 上高效运行这些模型。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-硬件">AI 硬件<a href="#ai-硬件" class="hash-link" aria-label="Direct link to AI 硬件" title="Direct link to AI 硬件">​</a></h3>
<ul>
<li>
<p>Qualcomm Kryo™ CPU- 一流的 CPU，具有高性能和卓越的能效。</p>
</li>
<li>
<p>Qualcomm Adreno GPU- 适合在需要平衡功耗与性能的情况下执行 AI 工作负载。AI 工作负载可以通过 OpenCL 内核进行加速。GPU 还可用于加速模型预处理/后处理。</p>
</li>
<li>
<p>Qualcomm Hexagon 张量处理器 (HTP)- 又称 NPU/DSP/HMX，适合以低功耗、高性能执行AI 工作负载。为优化性能，需要对预训练模型进行量化，使其达到支持的任一种精度。</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-软件">AI 软件<a href="#ai-软件" class="hash-link" aria-label="Direct link to AI 软件" title="Direct link to AI 软件">​</a></h3>
<p>AI 堆栈包含各种 SDK，以便利用 AI 硬件加速器的强大功能。开发人员可以使用自己选择的一种 SDK 来部署 AI 工作负载。预训练模型（TFLite 模型除外）在运行之前需要使用所选 SDK 将其转换为可执行格式。TFLite 模型可以使用 TFLite Delegate 直接运行。</p>
<ul>
<li>TFLite</li>
</ul>
<p>TFLite 模型可以在使用以下 Delegate 进行加速的条件下在 Qualcomm 硬件本地执行。</p>
<table><thead><tr><th>Delegate</th><th>加速</th></tr></thead><tbody><tr><td>AI Engine Direct Delegate (QNN Delegate)</td><td>CPU, GPU 和 HTP</td></tr><tr><td>XNNPACK Delegate</td><td>CPU</td></tr><tr><td>GPU Delegate</td><td>GPU</td></tr></tbody></table>
<ul>
<li>Qualcomm 神经网络处理引擎 (SNPE) SDK</li>
</ul>
<p>Qualcomm 神经网络处理引擎 (SNPE) 是一种用于执行深度神经网络的软件加速 runtime。SNPE 提供相关工具来对神经网络进行转换、量化，并在 CPU、GPU 和 HTP 等硬件加速器上对其进行加速。</p>
<ul>
<li>Qualcomm AI Engine Direct (QNN)</li>
</ul>
<p>Qualcomm AI Engine Direct 是 Qualcomm 芯片组和 AI 加速核心中适用于 AI/ML 用例场景的一种软件架构。该架构旨在提供统一的 API，模块和可扩展的预加速库，从而基于这种可重用的结构打造全栈 AI 解决方案。它可为 Qualcomm 神经网络处理 SDK、TFLite AI Engine Direct Delegate 等 runtime 提供支持。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="编译并优化模型">编译并优化模型<a href="#编译并优化模型" class="hash-link" aria-label="Direct link to 编译并优化模型" title="Direct link to 编译并优化模型">​</a></h2>
<p>用户可以采用两种可用途径中的任一种来编译和优化他们的模型。</p>
<ul>
<li>
<p>AI Hub</p>
</li>
<li>
<p>AI 软件堆栈</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-hub">AI Hub<a href="#ai-hub" class="hash-link" aria-label="Direct link to AI Hub" title="Direct link to AI Hub">​</a></h3>
<p>为了在 Qualcomm AI 硬件上快速构建模型原型，AI Hub 提供了一种方法，帮助开发人员针对视觉、音频和语音用例在设备上对机器学习模型进行优化、验证和部署。</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-178-01a8b66a3e850ea0014656828d94ac9a.jpg" width="954" height="242" class="img_ev3q"></p>
<p>有关设置和入门指南，可参见<a href="https://app.aihub.qualcomm.com/docs/hub/getting_started.html#installation" target="_blank" rel="noopener noreferrer"> AI Hub</a> 文档。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="环境配置">环境配置<a href="#环境配置" class="hash-link" aria-label="Direct link to 环境配置" title="Direct link to 环境配置">​</a></h4>
<ol>
<li>配置 Python 环境。</li>
</ol>
<p>在计算机上安装 <a href="https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html" target="_blank" rel="noopener noreferrer">miniconda</a>.</p>
<p><strong>Windows：</strong>安装完成后，通过 Start 菜单打开 Anaconda 提示符窗口。</p>
<p><strong>macOS/Linux：</strong>安装完成后，打开一个新的 shell 窗口。</p>
<p>为 Qualcomm(®) AI Hub 设置 Python 虚拟环境：</p>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">source &lt;path&gt;/miniconda3/bin/activate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda create python=3.8 -n qai_hub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda activate qai_hub</span><br></span></code></pre></div></div>
<ol start="2">
<li>安装 AI Hub Python 客户端。</li>
</ol>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install qai-hub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install &quot;qai-hub[torch]&quot;</span><br></span></code></pre></div></div>
<ol start="3">
<li>登录 AI Hub。</li>
</ol>
<p>前往 <a href="https://aihub.qualcomm.com/" target="_blank" rel="noopener noreferrer">AI Hub</a> 并使用 Qualcomm ID 登录，查看所创建作业的相关信息。</p>
<p>登录后，导航至 <strong>Account</strong>* *&gt; <strong>Settings</strong> &gt; <strong>API Token</strong>。此时应提供一个可用于配置客户端的 API 令牌。</p>
<ol start="4">
<li>在终端，使用以下命令通过 API 令牌配置客户端。</li>
</ol>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qai-hub configure --api_token &lt;INSERT_API_TOKEN&gt;</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ai-hub-工作流程">AI Hub 工作流程<a href="#ai-hub-工作流程" class="hash-link" aria-label="Direct link to AI Hub 工作流程" title="Direct link to AI Hub 工作流程">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="使用预优化模型">使用预优化模型<a href="#使用预优化模型" class="hash-link" aria-label="Direct link to 使用预优化模型" title="Direct link to 使用预优化模型">​</a></h5>
<p>导航到<a href="https://aihub.qualcomm.com/iot/models" target="_blank" rel="noopener noreferrer"> AI Hub Model Zoo</a>，访问适用于 RUBIK Pi 3 的模型.</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-175-f564911b1602242250b97e473b6204d5.jpg" width="1000" height="244" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-171-c49f169fa620565bf646d3472dfe42ca.jpg" width="1000" height="680" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-174-125f958b7195f5a93f07cc1978f3a19f.jpg" width="1000" height="691" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-170-21ade05e931c39315cdc1bcda63662f8.jpg" width="1000" height="664" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-173-8a14c64fbe746dcb82d07bf0c94d0b41.jpg" width="1000" height="682" class="img_ev3q"></p>
<p>点击下载按钮后开始模型下载。下载的模型已经过预先优化，可直接开发用户自己的应用程序。</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="引入用户自己的模型">引入用户自己的模型<a href="#引入用户自己的模型" class="hash-link" aria-label="Direct link to 引入用户自己的模型" title="Direct link to 引入用户自己的模型">​</a></h5>
<ol>
<li>
<p>选择 PyTorch 或 Onnx 格式的预训练模型。</p>
</li>
<li>
<p>使用 Python API 将模型提交至 AI Hub 以进行编译或优化。</p>
</li>
</ol>
<p>提交编译作业时，必须选择设备或芯片组以及目标 runtime 才能编译模型。RUBIK Pi 3 支持 TFLite runtime。</p>
<table><thead><tr><th>芯片组</th><th>Runtime</th><th>CPU</th><th>GPU</th><th>HTP</th></tr></thead><tbody><tr><td>QCS6490</td><td>TFLite</td><td>INT8,FP16, FP32</td><td>FP16,FP32</td><td>INT8,INT16</td></tr></tbody></table>
<p>提交后，AI Hub 会为该作业生成一个唯一的 ID。用户可以使用此作业 ID 查看作业详情。</p>
<ul>
<li>AI Hub 会根据选择的设备和 runtime 对模型进行优化。</li>
</ul>
<p>或者，也可以提交作业以在源自设备集群且已经过配置的实际设备上对优化模型进行分析或推理（使用 Python API）。</p>
<ul>
<li>
<p>性能分析：在已配置的设备上对模型进行基准测试并提供统计数据，包括层级的平均推理时间、runtime 配置等。</p>
</li>
<li>
<p>推理：在推理作业执行过程中，使用优化模型基于提交的数据进行推理，即在已配置的设备上运行该模型。</p>
</li>
</ul>
<ul>
<li>提交的每项作业都可以在 AI Hub 门户中进行审核。提交编译作业时，将提供优化模型的可下载链接。然后，该优化模型可以部署在 RUBIK Pi 3 等本地开发设备上。</li>
</ul>
<p>下方所示为从 <a href="https://aihub.qualcomm.com/iot/models" target="_blank" rel="noopener noreferrer">AI Hub documentation</a> 中获取的所述工作流程的示例。在本示例中，会将源自 PyTorch 的MobileNet V2 预训练模型上传到 AI Hub，并编译为优化的 TFLite 模型以便在 RUBIK Pi 3上运行。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> qai_hub </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> hub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> torchvision</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">models </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> mobilenet_v2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Using pre-trained MobileNet</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">torch_model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> mobilenet_v2</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">pretrained</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">torch_model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">eval</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Trace model (for on-device deployment)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_shape </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">example_input </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rand</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_shape</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">traced_torch_model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">jit</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">trace</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">torch_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> example_input</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Compile and optimize the model for a specific device</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">compile_job </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> hub</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">submit_compile_job</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">traced_torch_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">device</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">hub</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Device</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;QCS6490 (Proxy)&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_specs</span><span class="token operator" style="color:#393A34">=</span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">image</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">input_shape</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#compile_options=&quot;--target_runtime tflite&quot;,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Profiling Job</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">profile_job </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> hub</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">submit_profile_job</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">compile_job</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_target_model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">device</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">hub</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Device</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;QCS6490 (Proxy)&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sample </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">astype</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Inference Job</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inference_job </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> hub</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">submit_inference_job</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">compile_job</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_target_model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">device</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">hub</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Device</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;QCS6490 (Proxy)&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inputs</span><span class="token operator" style="color:#393A34">=</span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">image</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sample</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Download model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">compile_job</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">download_target_model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">filename</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;/tmp/mobilenetv2.tflite&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>要停用先前激活的 qai_hub 环境，可使用以下命令。</p>
<p><code>conda deactivate</code></p>
</blockquote></div></div>
<p>模型下载完毕后，即可直接开发用户自己的应用程序。</p>
<p>有关 AI Hub 工作流程和 API 的更多详细信息，可参见 <a href="https://app.aihub.qualcomm.com/docs/hub/index.html#examples" target="_blank" rel="noopener noreferrer">AI Hub Documentation</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tflite">TFLite<a href="#tflite" class="hash-link" aria-label="Direct link to TFLite" title="Direct link to TFLite">​</a></h3>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-172-93c8133f9ea2ca37c06448f4265a1c80.jpg" width="1000" height="587" class="img_ev3q"></p>
<p>TensorFlow Lite (TFLite) 是一个用于在设备上进行推理的开源深度学习框架。TFLite 可优化模型的延迟、模型尺寸、功耗等，帮助开发人员在移动、嵌入式和边缘平台上运行他们的模型。Qualcomm 支持通过下方列出的 TFLite Delegate 在 Qualcomm Linux 硬件本地执行 TFLite 模型。</p>
<table><thead><tr><th><strong>Delegate</strong></th><th><strong>加速</strong></th></tr></thead><tbody><tr><td>AI Engine Direct Delegate (QNN Delegate)</td><td>CPU, GPU，HTP</td></tr><tr><td>XNNPack Delegate</td><td>CPU</td></tr><tr><td>GPU Delegate</td><td>GPU</td></tr></tbody></table>
<table><thead><tr><th>参考指南</th><th>API 手册</th></tr></thead><tbody><tr><td><a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-70014-54/overview.html" target="_blank" rel="noopener noreferrer">Reference Guide</a></td><td><a href="https://www.tensorflow.org/lite/api_docs/cc?_gl=1*xotull*_ga*OTM5NzE4ODUwLjE2ODk2OTQ3MDI.*_ga_W0YLR4190T*MTcxNTM3NzQyMS4yNy4xLjE3MTUzNzc0MjEuMC4wLjA" target="_blank" rel="noopener noreferrer">C/C++</a></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="qualcomm-神经网络处理引擎">Qualcomm 神经网络处理引擎<a href="#qualcomm-神经网络处理引擎" class="hash-link" aria-label="Direct link to Qualcomm 神经网络处理引擎" title="Direct link to Qualcomm 神经网络处理引擎">​</a></h3>
<p>Qualcomm 神经网络处理引擎（又称 Snapdragon 神经网络处理引擎或 SNPE）是一款一站式解决方案 SDK，用于移植 ML 模型以在 Qualcomm 硬件加速器上运行。SNPE 提供相关工具对在PyTorch 和 TensorFlow 中经过训练的模型进行转换和量化，并提供 runtime 以便在 CPU、GPU 和 HTP 上执行这些模型。想要了解有关 SNPE SDK 的详细信息，可点击<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/introduction.html" target="_blank" rel="noopener noreferrer">此处</a>。</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-169-2295291f34544e163183a20923aa521f.jpg" width="880" height="841" class="img_ev3q"></p>
<table><thead><tr><th><strong>环境配置</strong></th><th><strong>定制</strong></th><th><strong>Run Inference</strong></th><th><strong>API Reference</strong></th></tr></thead><tbody><tr><td><a href="#installSDK">安装 Qualcomm 神经网络处理引擎 SDK</a><p><a href="#SetSDK">设置 Qualcomm 神经网络处理 SDK</a></p></td><td><a href="#modelconv">模型转换</a><p><a href="#modelquant">模型量化</a></p></td><td><a href="#modeldeploy">模型部署</a></td><td><a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/api.html" target="_blank" rel="noopener noreferrer">C/C++</a></td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="安装-qualcomm-神经网络处理引擎-sdk">安装 Qualcomm 神经网络处理引擎 SDK<a href="#安装-qualcomm-神经网络处理引擎-sdk" class="hash-link" aria-label="Direct link to 安装 Qualcomm 神经网络处理引擎 SDK" title="Direct link to 安装 Qualcomm 神经网络处理引擎 SDK">​</a></h4>
<a id="installSDK"></a>
<p>Qualcomm 神经网络处理引擎 SDK 需要在 Ubuntu 22.04 主机上运行。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>如果主机运行的是 Windows 或 macOS 操作系统软件，须按照<a href="https://qualcomm.com/DCN/80-70014-41Y/overview.html?product=1601111740013095" target="_blank" rel="noopener noreferrer">此处</a>提供的步骤安装虚拟机。后续步骤必须在运行 Ubuntu 22.04 LTS 的虚拟机中执行。</p>
</blockquote></div></div>
<p>Qualcomm 神经网络处理 SDK 工作流程已通过采用以下配置并运行于物理主机或虚拟机上的 Ubuntu 22.04 进行了验证。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>不支持在基于 Arm 架构的 macOS 操作系统中的 VM 内运行 Ubuntu 22.04。</p>
</blockquote></div></div>
<table><thead><tr><th><strong>下载方式</strong></th><th><strong>前提条件</strong></th><th><strong>可用版本</strong></th></tr></thead><tbody><tr><td><a href="#%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD">直接下载</a></td><td>无需任何前提条件即可下载</td><td>v2.22.6（SDK 每季度更新一次）</td></tr><tr><td><a href="#qualcomm-package-manager">Qualcomm Package Manager</a></td><td>需要拥有有效的 Qualcomm ID。Qualcomm Package Manager 工具。</td><td>SDK 每月更新一次</td></tr></tbody></table>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="直接下载">直接下载<a href="#直接下载" class="hash-link" aria-label="Direct link to 直接下载" title="Direct link to 直接下载">​</a></h5>
<p>Qualcomm 神经网络处理引擎 SDK 可从<a href="https://softwarecenter.qualcomm.com/api/download/software/qualcomm_neural_processing_sdk/v2.22.6.240515.zip" target="_blank" rel="noopener noreferrer">此处</a>直接下载。下载后，提取或解压 SDK。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>上述链接提供的 SDK 版本每季度更新一次。</p>
</blockquote></div></div>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">unzip 2.22.6.240515.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd qairt/2.22.6.240515/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export SNPE_ROOT=`pwd`</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="qualcomm-package-manager">Qualcomm Package Manager<a href="#qualcomm-package-manager" class="hash-link" aria-label="Direct link to Qualcomm Package Manager" title="Direct link to Qualcomm Package Manager">​</a></h5>
<p>Qualcomm 神经网络处理引擎 SDK 也可以通过 Qualcomm Package Manager (QPM) 下载。本部分将演示如何使用 QPM 下载 Qualcomm 神经网络处理引擎 SDK。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>如需使用 Qualcomm Package Manager 下载 Qualcomm 神经网络处理 SDK，应确保用户已经注册了 Qualcomm ID。如果用户没有 Qualcomm ID，系统将提示其注册。然后，按照下方说明下载并安装 SDK。  <strong>         </strong></p>
</blockquote></div></div>
<ol>
<li>
<p>前往 <a href="https://qpm.qualcomm.com/#/main/" target="_blank" rel="noopener noreferrer">Qualcomm Package Manager</a> 并使用 Qualcomm ID 登录。</p>
</li>
<li>
<p>导航至 <strong>Tools </strong>选项卡并在左侧窗格中搜索 &quot;AI Stack&quot;。在筛选结果中，点击 <strong>Qualcomm® AI Stack </strong>旁边的箭头，展开可用的 AI 软件 SDK 列表，然后从列表中选择 <strong>Qualcomm Neural Processing SDK</strong>。</p>
</li>
</ol>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-177-9f56d233d1d77ac319733401867ed407.jpg" width="1000" height="364" class="img_ev3q"></p>
<ol start="3">
<li>点击 <strong>Qualcomm Neural Processing SDK</strong>，导航至下一页。从下拉列表中选择 <strong>Linux </strong>和 <strong>Version 2.22.6.2405155</strong>，然后点击 <strong>Download </strong>下载 Qualcomm 神经网络处理 SDK 安装程序。</li>
</ol>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-166-a619f0da20c81c34a9c420e2f5fbe9ee.jpg" width="1000" height="467" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<ul>
<li>
<p>这些步骤基于版本 2.22.6.240515。</p>
</li>
<li>
<p>如果使用 Qualcomm Package Manager 桌面工具，<strong>Extract </strong>将取代下载按钮。点击该按钮会自动安装 SNPE SDK。</p>
</li>
</ul>
</blockquote></div></div>
<ul>
<li>
<p>安装使用 QPM CLI 工具下载的安装程序 (.qik)。</p>
<ol>
<li>使用 QPM CLI 登录。</li>
</ol>
</li>
</ul>
<p>            <code>qpm-cli --login &lt;username&gt; </code>         </p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-168-4bacce79fe1556e3ab0ec2fdd9d79424.jpg" width="1000" height="162" class="img_ev3q"></p>
<p>          <code>qpm-cli --license-activate qualcomm_neural_processing_sdk</code></p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-167-91ce525ae6a02e503aba97753fd2d1a3.jpg" width="1000" height="63" class="img_ev3q"></p>
<p> </p>
<p>          <code>qpm-cli --extract &lt;path to downloaded .qik file&gt;</code></p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-176-fd0a1226e604600685223c9f9c0b3150.jpg" width="1000" height="267" class="img_ev3q"></p>
<p>Qualcomm 神经网络处理 SDK 将安装在以下路径：</p>
<p><em>/opt/qcom/aistack/qairt/2.22.6.240515</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="设置-qualcomm-神经网络处理-sdk">设置 Qualcomm 神经网络处理 SDK<a href="#设置-qualcomm-神经网络处理-sdk" class="hash-link" aria-label="Direct link to 设置 Qualcomm 神经网络处理 SDK" title="Direct link to 设置 Qualcomm 神经网络处理 SDK">​</a></h4>
<a id="SetSDK"></a>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="前提条件"><strong>前提条件</strong><a href="#前提条件" class="hash-link" aria-label="Direct link to 前提条件" title="Direct link to 前提条件">​</a></h5>
<ol>
<li>主机操作系统：Ubuntu22.04 LTS</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>如果主机运行的是 Windows 或 Mac OS 操作系统，须按照<a href="https://qualcomm.com/DCN/80-70014-41Y/overview.html?product=1601111740013095" target="_blank" rel="noopener noreferrer">此处</a>提供的步骤安装虚拟机。后续步骤必须在运行 Ubuntu 22.04 LTS 的虚拟机中执行。</p>
</blockquote></div></div>
<ol start="2">
<li>
<p>在 SELinux Permissive 模式下启用 SSH 以安全地登录主机设备。相关说明，可参见 <a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-70014-254Y/how_to.html#-ssh-" target="_blank" rel="noopener noreferrer">SSH 使用指南</a>。</p>
</li>
<li>
<p>对于本文档的其余部分，环境变量 SNPE_ROOT 表示 Qualcomm® 神经网络处理 SDK 根目录的完整路径。</p>
</li>
</ol>
<p>如果通过直接下载的方式安装，${SNPE_ROOT} 表示解压后的 SDK 所在路径。</p>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">unzip v2.22.6.240515.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/qairt/2.22.6.240515</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export SNPE_ROOT=`pwd`</span><br></span></code></pre></div></div>
<p>如果通过 QPM 安装，则 SDK 安装在以下路径：<em>/opt/qcom/aistack/qairt/&lt;version&gt;</em></p>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export SNPE_ROOT=/opt/qcom/aistack/qairt/2.22.6.240515/</span><br></span></code></pre></div></div>
<ol start="4">
<li>Python: v3.10</li>
</ol>
<p>如果已安装 Python，确保环境路径已更新为 Python 3.10 的路径。如果系统中未安装 Python 3.10，可以使用以下命令进行安装：</p>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt-get update</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt-get install python3.10 python3-distutils libpython3.10</span><br></span></code></pre></div></div>
<ol start="5">
<li>系统依赖项：</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>以管理员/root 身份运行以下命令，安装系统库。          <strong> </strong></p>
</blockquote></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo bash $</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">SNPE_ROOT</span><span class="token punctuation" style="color:#393A34">}</span><span class="token operator" style="color:#393A34">/</span><span class="token builtin">bin</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">check</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">linux</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">dependency</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">sh</span><br></span></code></pre></div></div>
<ol start="6">
<li>虚拟环境 (VENV)</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p><em>&lt;venv_path&gt; </em>表表示新虚拟环境的路径。</p>
</blockquote></div></div>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt-get install python3.10-venv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3.10 -m venv &quot;&lt;venv_path&gt;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source &lt;venv_path&gt;/bin/activate</span><br></span></code></pre></div></div>
<p>运行以下脚本检查并安装缺少的依赖项：</p>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 -m pip install --upgrade pip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">${SNPE_ROOT}/bin/check-python-dependency</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-164-a9d79ded8374dde0fa34c6cd70c1e263.jpg" width="308" height="393" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="设置-ml-框架"><strong>设置 ML 框架</strong><a href="#设置-ml-框架" class="hash-link" aria-label="Direct link to 设置-ml-框架" title="Direct link to 设置-ml-框架">​</a></h5>
<p>为了将在其他框架上训练的 ML 模型转换为 Qualcomm® 神经网络处理 SDK 可用的模型文件，可能需要将相应框架下载到主机上并进行安装。</p>
<p>此 Qualcomm® 神经网络处理 SDK 版本已经过验证，可与以下版本的 ML 训练框架配合使用：</p>
<table><thead><tr><th><strong>框架</strong></th><th><strong>版本</strong></th></tr></thead><tbody><tr><td>TensorFlow</td><td>v2.10.1</td></tr><tr><td>TFLite</td><td>v2.3.0</td></tr><tr><td>PyTorch</td><td>V1.13.1</td></tr><tr><td>ONNX</td><td>V1.12.0</td></tr></tbody></table>
<div class="language-cmake codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cmake codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Install tensorflow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install tensorflow==2.10.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Install tflite</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install tflite==2.3.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Install PyTorch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install torch==1.13.1+cpu torchvision==0.14.1+cpu torchaudio==0.13.1 --</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">extra-index-url https://download.pytorch.org/whl/cpu</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Install Onnx</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install onnx==1.12.0 onnxruntime==1.17.1 onnxsim==0.4.36</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="配置-snpe-sdk-环境"><strong>配置 SNPE SDK 环境</strong><a href="#配置-snpe-sdk-环境" class="hash-link" aria-label="Direct link to 配置-snpe-sdk-环境" title="Direct link to 配置-snpe-sdk-环境">​</a></h5>
<p>导入 SNPE SDK 提供的环境配置脚本，确保所有必要的工具和库均已提供在 $PATH 中。</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">source ${SNPE_ROOT}/bin/envsetup.sh</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="使用-snpe-移植模型">使用 SNPE 移植模型<a href="#使用-snpe-移植模型" class="hash-link" aria-label="Direct link to 使用 SNPE 移植模型" title="Direct link to 使用 SNPE 移植模型">​</a></h4>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-165-c8b5fd7c8617dccb000d5bc529e68398.jpg" width="1000" height="494" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="模型转换">模型转换<a href="#模型转换" class="hash-link" aria-label="Direct link to 模型转换" title="Direct link to 模型转换">​</a></h5>
<a id="modelconv"></a>
<p>将源自 PyTorch、Onnx、TensorFlow 或 TFLite 的 32 位精度浮点型预训练模型输入到 SNPE 转换工具 (snpe-&lt;framework&gt;-to-dlc)，将模型转换为 Qualcomm 特定的模型的模型文件，即深度学习容器 (DLC)。</p>
<p>除了来自源框架的输入模型之外，转换工具还需要有关输入模型的其他详细信息，例如输入节点名称、其对应的输入维度以及任何输出张量名称（对于具有多个输出的模型而言）。</p>
<p>可参考<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/tools.html" target="_blank" rel="noopener noreferrer">转换工具</a>部分，了解所有可用的可配置参数；也可以运行 <code>snpe-&lt;framework&gt;-to-dlc --help</code>，查看命令行帮助。</p>
<div class="language-plain text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-plain codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">required arguments:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        The names and dimensions of the network input layers specified in the format</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        [input_name comma-separated-dimensions], for example:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                   &#x27;data&#x27; 1,224,224,3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        Note that the quotes should always be included in order to handle special</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                       characters, spaces, etc.For multiple inputs specify multiple --input_dim on the command line</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        like:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                   --input_dim &#x27;data1&#x27; 1,224,224,3 --input_dim &#x27;data2&#x27; 1,50,100,3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    --out_node OUT_NAMES, --out_name OUT_NAMES</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        Name of the graph&#x27;s output Tensor Names. Multiple output names should be provided separately                            like:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                    --out_name out_1 --out_name out_2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    --input_network INPUT_NETWORK, -i INPUT_NETWORK</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                    Path to the source framework model.</span><br></span></code></pre></div></div>
<p>以下示例使用的是从 <a href="https://github.com/onnx/models/blob/main/Computer_Vision/inception_v3_Opset16_timm/inception_v3_Opset16.onnx" target="_blank" rel="noopener noreferrer">ONNX Model Zoo </a>下载的 ONNX 模型 (<em>inception_v3_opset16.onnx</em>)。将模型以 inception_v3.onnx 另存到工作区中。在本示例中，我们将模型下载到 <em>~/models</em> 目录。</p>
<p>运行以下命令，生成 inception_v3.dlc<strong> </strong>模型。</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">${SNPE_ROOT}/bin/x86_64-linux-clang/snpe-onnx-to-dlc --input_network ~/models/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3.onnx --output_path ~/models/inception_v3.dlc --input_dim &#x27;x&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1,3,299,299</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-193-04f8f407ae53ef04283adb17496391c6.jpg" width="1000" height="553" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="模型量化">模型量化<a href="#模型量化" class="hash-link" aria-label="Direct link to 模型量化" title="Direct link to 模型量化">​</a></h5>
<a id="modelquant"></a>
<p>如需在 Hexagon 张量处理器 (HTP) 上运行模型，转换后的 DLC 必须进行量化。SNPE 提供了一个量化工具 (snpe-dlc-quant)，可使用其自身的量化算法将 DLC 模型量化为 INT8/INT16 DLC。有关 SNPE 量化的更多信息，可从<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/quantized_models.html" target="_blank" rel="noopener noreferrer">此处</a>获取。</p>
<p>SNPE 中的量化过程分为两个步骤：</p>
<ol>
<li>模型内部权重和偏置的量化。</li>
</ol>
<p>权重和偏置的量化是一个静态步骤，即无需用户额外输入数据。</p>
<ol start="2">
<li>激活层（或没有权重的层）的量化。</li>
</ol>
<ul>
<li>
<p>对激活层进行量化时，需要将一组来自训练数据集的输入图像用作校准数据。</p>
</li>
<li>
<p>这些校准数据集图像采用 .raw 格式，以预处理图像文件列表形式输入。这些输入的 .raw 文件的文件大小必须与模型的输入大小相匹配。</p>
</li>
</ul>
<p><code>snpe-dlc-quant</code> 的输入包括一个转换后的 DLC 模型和一个包含校准数据集图像路径的纯文本文件。该输入列表包含以 NumPy 数组形式保存在 .raw 格式文件中的预处理图像的路径。预处理图像的大小必须与模型的输入分辨率相匹配。</p>
<p><code>Snpe-dlc-quant </code>工具的输出是一个量化后的 DLC。</p>
<div class="language-plain text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-plain codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[ --input_dlc=&lt;val&gt; ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        Path to the dlc container</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        containing the model for which fixed-point encoding metadata should be generated.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        This argument is required.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">             [ --input_list=&lt;val&gt; ] Path to a file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         specifying the trial inputs. This file should be a plain text file, containing one or more absolute file                            paths per line. These files will be taken to constitute the trial set. Each path is expected to point to                           a binary file containing one trial input in the &#x27;raw&#x27; format, ready to be consumed by the tool                                        without any further modifications. This is similar to how input is provided to snpe-net-run                                          application.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">             [ --output_dlc=&lt;val&gt; ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         Path at which the metadata-included quantized model container should be  written. If this                                          argument is omitted, the quantized model will be written at                                                                                                         &lt;unquantized_model_name&gt;_quantized.dlc.</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>  使用 <a href="https://github.com/lutzroeder/netron/releases/latest" target="_blank" rel="noopener noreferrer">Netron </a>图形可视化工具来识别模型的输入/输出层维度。      </p>
</blockquote></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-192-0ccb7e7b25eb2fd4f0f1f0dec0f486b8.jpg" width="1000" height="382" class="img_ev3q"></p>
<p>为进行演示，我们可以使用随机输入文件来评估量化过程。可以使用下方所示的简单 Python 脚本生成 inception_v3.onnx 模型的输入文件。将脚本以 <em>generate_random_input.py</em> 文件形式保存在工作区 <em>~/models/</em> 中，并使用 <em>python ~/models/generate_random_input.py</em> 在主机上运行该脚本。</p>
<p>以下 Python 代码示例可创建一个 input_list，其中包含用于量化模型的校准数据集图像的路径。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_path_list </span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BASE_PATH </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/tmp/RandomInputsForInceptionV3&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">not</span><span class="token plain"> os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">path</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">exists</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">BASE_PATH</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mkdir</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">BASE_PATH</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># generate 10 random inputs and save as raw</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_IMAGES </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#binary files</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> img </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">NUM_IMAGES</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    filename </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;input_{}.raw&quot;</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">format</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">img</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    randomTensor </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">299</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">299</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">astype</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    filename </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">path</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">join</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">BASE_PATH</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> filename</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    randomTensor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">tofile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">filename</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    input_path_list</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">filename</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#for saving as input_list text</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">with</span><span class="token plain"> </span><span class="token builtin">open</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;input_list.txt&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;w&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> f</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> path </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> input_path_list</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        f</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">write</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">path</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        f</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">write</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">‘\n’</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>上述脚本会生成 10 个示例输入文件并保存在 <em>/tmp/RandomInputsForInceptionV3</em> 目录，以及一个 <em>input_list.txt </em>文件，其中包含所生成的每个示例的路径。</p>
<p>现在，snpe-dlc-quant 工具所需的所有输入均已准备好，现在可以对模型进行量化了。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">SNPE_ROOT</span><span class="token punctuation" style="color:#393A34">}</span><span class="token operator" style="color:#393A34">/</span><span class="token builtin">bin</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">x86_64</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">linux</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">clang</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">snpe</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">dlc</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">quant </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">input_dlc </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dlc </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">output_dlc </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">inception_v3_quantized</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dlc </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_list </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">input_list</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">txt</span><br></span></code></pre></div></div>
<p>这会生成量化后的 inception_v3 DLC 模型 (inception_v3_quantized.dlc)。默认情况下，对模型进行量化时采用 INT8 位宽。</p>
<p>开发人员可以指定 snpe-dlc-quant 工具的 [--act_bw 16] 和/或 [--weight_bw 16] 选项，将量化定制为使用 16 位位宽，而不用默认的 INT8 位宽。</p>
<p>可参见 <a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/tools.html" target="_blank" rel="noopener noreferrer">snpe-dlc-quant </a>工具文档或运行 <code>snpe-dlc-quant --help</code>，查看所有可用的定制选项，包括量化模式、优化等。</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-191-f5dad013167dcdb4ebfc3164fd24246f.jpg" width="1000" height="712" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="模型优化">模型优化<a href="#模型优化" class="hash-link" aria-label="Direct link to 模型优化" title="Direct link to 模型优化">​</a></h5>
<p>量化模型的 DLC 需要执行一个图形准备步骤，该步骤会对模型进行优化，以便在 HTP 上执行。为完成模型 DLC 的准备工作以便在 HTP 上执行，SNPE 提供了 snpe-dlc-graph- prepare 工具，工具的输入参数为量化模型和硬件特定细节，例如芯片型号。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>对硬件（例如 HTP）的优化取决于芯片组上 HTP 的具体版本。为确保神经网络对执行图应用正确的优化指令集，从而优化 HTP 的利用率，务必向 snpe-dlc- graph-prepare 工具提供正确的芯片组 ID。</p>
</blockquote></div></div>
<p>该工具会根据 HTP 版本和芯片组 ID 创建一个缓存，其中存储在 HTP 硬件上执行模型 DLC 的执行策略。如果省去这一步，在网络初始化期间会产生额外的开销，因为 SNPE runtime 必须动态创建执行策略。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">SNPE_ROOT</span><span class="token punctuation" style="color:#393A34">}</span><span class="token operator" style="color:#393A34">/</span><span class="token builtin">bin</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">x86_64</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">linux</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">clang</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">snpe</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">dlc</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">graph</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">prepare </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">input_dlc </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">inception_v3_quantized</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dlc </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">output_dlc </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3_quantized_with_htp_cache</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dlc </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">htp_socs qcs6490</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-190-74b2cb60cb20a87bcbf264f566e1ebdb.jpg" width="1000" height="372" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="htp-缓存信息">HTP 缓存信息<a href="#htp-缓存信息" class="hash-link" aria-label="Direct link to HTP 缓存信息" title="Direct link to HTP 缓存信息">​</a></h5>
<p>snpe-dlc-graph-prepare 步骤完成后，会将 HTP 缓存记录添加到 DLC 中。缓存信息可以使用 snpe-dlc-info 工具查看。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">SNPE_ROOT</span><span class="token punctuation" style="color:#393A34">}</span><span class="token operator" style="color:#393A34">/</span><span class="token builtin">bin</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">x86_64</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">linux</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">clang</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">snpe</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">dlc</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">info </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">i </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3_quantized_with_htp_cache</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dlc</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-188-cce47a034dfe2a7f543330feec5fc473.jpg" width="1000" height="550" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="模型部署">模型部署<a href="#模型部署" class="hash-link" aria-label="Direct link to 模型部署" title="Direct link to 模型部署">​</a></h5>
<a id="modeldeploy"></a>
<p>模型 DLC（量化或非量化）可以通过支持 SNPE 的应用程序（使用 SNPE C/C++ API 编写的应用程序）进行部署。SNPE 提供相关 API 来加载 DLC、选择 runtime，从而执行模型以及执行推理等。</p>
<p>SNPE 提供预编译的 <a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/tools.html#snpe-net-run" target="_blank" rel="noopener noreferrer">snpe-net-run </a>工具（使用 C API 编写的应用程序），该工具可以加载任意模型 DLC 并根据提供的输入进行执行。</p>
<ul>
<li>
<p><strong>模型文件</strong>- 由 SNPE 转换工具或 snpe-dlc-graph-prepare 工具（如果在 HTP 上运行）生成的 DLC 模型文件。</p>
</li>
<li>
<p><strong>输入列表</strong>- 文本文件，与量化时使用的<em> input_list.txt </em>文件类似，但此列表中的输入原始文件将用于推理。在本示例中，为简单起见，将量化时使用的同一输入列表重用于推理。</p>
</li>
<li>
<p><strong>Runtime- </strong>用户必须选择具体的 runtime 才能在目标设备上执行模型。可用的 runtime 选项包括 CPU、GPU 和 DSP (HTP)。</p>
</li>
</ul>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>有关详细详细，可参见 <code>snpe-net-run --help</code>。</p>
</blockquote></div></div>
<p><strong>在 x86 主机上运行 SNPE DLC</strong></p>
<p>转换后的 SNPE DLC 可以使用 snpe-net-run 工具运行，该工具将 DLC 和 input_list 用作参数。以下命令会生成输出文件并保存到<em> /output_x86/ </em>目录下。在 x86 主机上运行 SNPE DLC 只是为了完成调试。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>SNPE 中的默认 runtime 是 CPU。使用 snpe-net-run 执行模型时，无需指定 CPU runtime。</p>
</blockquote></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">SNPE_ROOT</span><span class="token punctuation" style="color:#393A34">}</span><span class="token operator" style="color:#393A34">/</span><span class="token builtin">bin</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">x86_64</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">linux</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">clang</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">snpe</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">net</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">run </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">container </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dlc </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">input_list </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">input_list</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">txt </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">output_dir </span><span class="token operator" style="color:#393A34">~</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">models</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output_x86</span><br></span></code></pre></div></div>
<p>以上命令会加载模型 DLC 并在 x86 CPU 上执行该模型。模型执行后获得的输出将写入 <em>output_x86</em> 目录。</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-189-eabcf1a6fd05854b1bb1a010adb77f26.jpg" width="1000" height="589" class="img_ev3q"></p>
<p><strong>准备 SNPE 模型以在目标设备上运行</strong></p>
<p>如需在目标设备上运行模型，snpe-net-run 需要使用模型 DLC、SNPE runtime 库和输入列表来运行推理，从而生成输出。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>在目标设备上运行之前，要确保 SNPE SDK 二进制文件和库已推送到目标设备中。</p>
</blockquote></div></div>
<p>对于 RUBIK Pi 3，使用路径 <em>${SNPE_ROOT}/lib/aarch64-oe-linux-gcc11.2</em> 和 <em>${SNPE_ROOT}/bin/aarch64-oe-linux-gcc11.2</em>.<em> </em>中的以下文件。</p>
<table><thead><tr><th><strong>文件</strong></th><th><strong>源位置</strong></th></tr></thead><tbody><tr><td>snpe-net-run</td><td>${SNPE_ROOT}/bin/aarch64-oe-linux-gcc11.2</td></tr><tr><td>libSNPE.so</td><td>${SNPE_ROOT}/lib/aarch64-oe-linux-gcc11.2/</td></tr><tr><td>libSnpeHtpPrepare.so</td><td>${SNPE_ROOT}/lib/aarch64-oe-linux-gcc11.2</td></tr><tr><td>libSnpeHtpV68Stub.so</td><td>${SNPE_ROOT}/lib/aarch64-oe-linux-gcc11.2</td></tr><tr><td>libSnpeHtpV68Skel.so</td><td>${SNPE_ROOT}/lib/hexagon-v68/unsigned</td></tr><tr><td>Inception_v3_quantized_with_htp_cache.dlc</td><td>~/models</td></tr><tr><td>Inception_v3.dlc</td><td>~/models</td></tr><tr><td>input_list.txt</td><td>~/models</td></tr><tr><td>Inception V3 sample input images</td><td>/tmp/RandomInputsForInceptionV3</td></tr></tbody></table>
<p>下列 scp 命令需要在主机上执行,以将 SNPE SDK 库和二进制文件复制到设备。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">scp ${SNPE_ROOT}/bin/aarch64-oe-linux-gcc11.2/snpe-net-run root@[ipaddr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ${SNPE_ROOT}/lib/aarch64-oe-linux-gcc11.2/libSNPE.so root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ${SNPE_ROOT}/lib/aarch64-oe-linux-gcc11.2/libSnpeHtpV68Stub.so root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ${SNPE_ROOT}/lib/aarch64-oe-linux-gcc11.2/libSnpeHtpPrepare.so root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ${SNPE_ROOT}/lib/hexagon-v68/unsigned/libSnpeHtpV68Skel.so root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ~/models/inception_v3.dlc root@[ip- addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ~/models/inception_v3_quantized_with_htp_cache.dlc root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ~/models/input_list.txt root@[ip- addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp /tmp/RandomInputsForInceptionV3 root@[ip- addr]:/tmp/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ssh root@[ip-addr]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd /opt</span><br></span></code></pre></div></div>
<p>上述步骤为设备执行模型做好了准备。以下部分详细介绍如何在可用 runtime  中运行模型。</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="在-arm-cpu-上运行-snpe-模型">在 Arm CPU 上运行 SNPE 模型<a href="#在-arm-cpu-上运行-snpe-模型" class="hash-link" aria-label="Direct link to 在 Arm CPU 上运行 SNPE 模型" title="Direct link to 在 Arm CPU 上运行 SNPE 模型">​</a></h5>
<p>首先设置环境变量然后再执行模型，以确保在运行模型时 SNPE  二进制文件和库可以访问。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>SNPE 中的默认 runtime 是 CPU。使用 snpe-net-run 执行模型时，无需指定 CPU runtime。</p>
</blockquote></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export LD_LIBRARY_PATH</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">$LD_LIBRARY_PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export PATH</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token operator" style="color:#393A34">/</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">$PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">snpe</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">net</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">run </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">container inception_v3</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dlc </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">input_list input_list</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">txt </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output_dir output_cpu</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-186-3e1facd9d0339267235aeed1d7fc0518.jpg" width="1000" height="578" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="在-gpu-上运行-snpe-模型">在 GPU 上运行 SNPE 模型<a href="#在-gpu-上运行-snpe-模型" class="hash-link" aria-label="Direct link to 在 GPU 上运行 SNPE 模型" title="Direct link to 在 GPU 上运行 SNPE 模型">​</a></h5>
<p>如需在 GPU runtime 上运行模型，可使用 inception_v3.dlc 并将输出保存在 <em>output_gpu </em>目录下。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>如需在 GPU 上运行模型，可通过 <code>--use_gpu</code> 命令行参数指定 runtime。      <strong>       </strong></p>
</blockquote></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export LD_LIBRARY_PATH</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">$LD_LIBRARY_PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export PATH</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">$PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">snpe</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">net</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">run </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">container inception_v3</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dlc </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">input_list input_list</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">txt </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output_dir output_gpu </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">use_gpu</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="在-htp-上运行-snpe-模型">在 HTP 上运行 SNPE 模型<a href="#在-htp-上运行-snpe-模型" class="hash-link" aria-label="Direct link to 在 HTP 上运行 SNPE 模型" title="Direct link to 在 HTP 上运行 SNPE 模型">​</a></h5>
<p>如需在 HTP 后端运行模型，可使用 inception_v3_quantized_with_htp_cache.dlc 模型 DLC 并将输出保存在 <em>output_htp</em> 目录。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>如需在 HTP 上运行模型，可通过 <code>--use_dsp</code> 命令行参数指定 runtime。              </p>
</blockquote></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export LD_LIBRARY_PATH=/opt:$LD_LIBRARY_PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export PATH=/opt:$PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export ADSP_LIBRARY_PATH=&quot;/opt;/usr/lib/rfsa/adsp;/dsp&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">snpe-net-run --container inception_v3_quantized_with_htp_cache.dlc --</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_list input_list.txt --output_dir output_htp --use_dsp</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-185-ab61ba994a3c1ab93a46b0db3f76f9a0.jpg" width="1000" height="368" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="对输出进行验证">对输出进行验证<a href="#对输出进行验证" class="hash-link" aria-label="Direct link to 对输出进行验证" title="Direct link to 对输出进行验证">​</a></h5>
<p>对于每个送入 snpe-net-run（通过 input_list 文件）的输入原始文件，都会生成一个输出文件夹，其中存储以原始文件形式保存的输出张量，文件大小将与模型的输出层相匹配，如下图所示。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p><a href="https://netron.app/" target="_blank" rel="noopener noreferrer">Netron </a>工具用于对模型进行可视化。</p>
</blockquote></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-187-385a8a7d62f60b2114656dfaecdf5ca0.jpg" width="1000" height="559" class="img_ev3q"></p>
<p>在示例 inception_v3 中，输出原始文件是一个二进制文件，其中存储 1000 个分类类别的概率。</p>
<p>我们使用 Python 脚本将文件读取为 NumPy 数组，以执行后处理步骤，并对输出进行验证。下方示例用于检查 HTP 预测结果是否与 CPU 预测结果相同。</p>
<p>将输出文件从目标设备复制到主机，以便对输出进行验证。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ssh root@</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">ip</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">addr</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd </span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">r </span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">output_cpu user@host</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">ip</span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">path</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">r </span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">output_htp user@host</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">ip</span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">path</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>在下方 (compare.py) Python 脚本中，须使用以上命令中的 <em>&lt;path&gt;</em>。</p>
</blockquote></div></div>
<p>将设备上推理的输出复制到主机后，准备一个脚本来加载保存在 output_htp 和 output_cpu 中的输出张量以进行比较。在本示例中，需要将 output_htp 和 output_cpu 都复制到 <em>~/ models</em> 目录。</p>
<p>下方示例展示了对特定输入如何比较输出结果。在本示例中，输出结果保存到主机上的 <em>/opt </em>目录。snpe-net-run 的输出结果可以使用 numpy.fromfile(…) API 加载成 NumPy 多维数组。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>默认情况下，snpe-net-run 会将输出张量保存为 float32<strong> </strong>格式的 NumPy 文件。</p>
</blockquote></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># python postprocessing script (compare.py)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">htp_output_file_path </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;&lt;path&gt;/output_htp/Result_1/875.raw&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cpu_output_file_path </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;&lt;path&gt;/output_cpu/Result_1/875.raw&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">htp_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">fromfile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">htp_output_file_path</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">htp_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> htp_output</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">reshape</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cpu_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">fromfile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cpu_output_file_path</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cpu_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cpu_output</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">reshape</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># np.argmax gives the cls_id with highest probability from tensor.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cls_id_htp </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">argmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">htp_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cls_id_cpu </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">argmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cpu_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Let&#x27;s compare CPU output vs HTP output</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;Cpu prediction {} \n Htp Prediction {}&quot;</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">format</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cls_id_cpu</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> cls_id_htp</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>输出</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-183-afeed2a67c935b96a0f132c3ad04b894.jpg" width="1000" height="187" class="img_ev3q"></p>
<p><strong> 使用 SNPE API 部署模型</strong></p>
<p>SNPE SDK 提供 C/C++ API，用于创建/开发应用程序，以在所选硬件（CPU、GPU 或 HTP）上加速运行模型。<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/cplus_plus_tutorial.html" target="_blank" rel="noopener noreferrer">此处</a>是一个使用 SNPE C/C++ API 开发的用于执行模型的示例程序。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="qualcomm-ai-engine-direct-sdk">Qualcomm AI Engine Direct SDK<a href="#qualcomm-ai-engine-direct-sdk" class="hash-link" aria-label="Direct link to Qualcomm AI Engine Direct SDK" title="Direct link to Qualcomm AI Engine Direct SDK">​</a></h3>
<p>Qualcomm AI Engine Direct SDK（也称为 QNN SDK）提供 low-level API，用以在 Qualcomm 平台上实现 AI 的最佳性能。QNN SDK 支持来自 PyTorch、TensorFlow 和 Onnx 等框架的模型。QNN SDK 提供离线工具对模型进行转换、量化和优化，并在 Qualcomm AI 加速引擎上部署。</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-182-a3cd813e9ba2a2cab36062d72c566f7b.jpg" width="1000" height="571" class="img_ev3q"></p>
<table><thead><tr><th><strong>环境配置</strong></th><th><strong>定制</strong></th><th><strong>运行推理</strong></th><th><strong>API 手册</strong></th></tr></thead><tbody><tr><td><a href="#%E5%AE%89%E8%A3%85-ai-engine-direct">安装 AI Engine Direct</a><p><a href="#%E8%AE%BE%E7%BD%AE-qualcomm-ai-engine-direct">设置 Qualcomm AI Engine Direct</a> </p></td><td><a href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%8E%E9%87%8F%E5%8C%96">模型转化与量化</a><p><a href="#%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%AF%91">模型编译</a> </p></td><td><a href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2">部署模型</a></td><td><a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/api.html" target="_blank" rel="noopener noreferrer">C/C++</a></td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="安装-ai-engine-direct">安装 AI Engine Direct<a href="#安装-ai-engine-direct" class="hash-link" aria-label="Direct link to 安装 AI Engine Direct" title="Direct link to 安装 AI Engine Direct">​</a></h4>
<p>Qualcomm AI Engine Direct SDK 需要在 Ubuntu 22.04 主机上运行。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>如果主机运行的是 Windows 或 macOS 操作系统软件，须按照 <a href="/linux120.github.io/docs/set-up-development-environment#windows-11-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">Windows 11 开发环境搭建</a>、 <a href="/linux120.github.io/docs/set-up-development-environment#mac-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAarm64">Mac 开发环境搭建（Arm64）</a>、 <a href="/linux120.github.io/docs/set-up-development-environment#mac-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAx86_64">Mac 开发环境搭建（x86_64）</a>提供的步骤安装虚拟机。后续步骤必须在运行 Ubuntu 22.04 LTS 的虚拟机中执行。        <strong>   </strong></p>
</blockquote></div></div>
<p>Qualcomm AI Engine Direct SDK 工作流程已通过采用以下配置进行了验证：运行于物理主机或虚拟机上的 Ubuntu 22.04。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>不支持在基于 Arm 架构的 macOS 操作系统中的 VM 内运行 Ubuntu 22.04。</p>
</blockquote></div></div>
<table><thead><tr><th><strong>下载方式</strong></th><th><strong>前提条件</strong></th><th><strong>可用版本</strong></th></tr></thead><tbody><tr><td><a href="#%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD-1">直接下载</a></td><td>无需任何前提条件即可下载。</td><td>v2.22.6（SDK 每季度更新一次）</td></tr><tr><td><a href="#qualcomm-package-manager-1">Qualcomm Package Manager</a></td><td>需要拥有有效的 Qualcomm ID。Qualcomm Package Manager 工具。</td><td>SDK 每月更新一次。</td></tr></tbody></table>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="直接下载-1">直接下载<a href="#直接下载-1" class="hash-link" aria-label="Direct link to 直接下载" title="Direct link to 直接下载">​</a></h5>
<p>AI Engine Direct SDK 可从<a href="https://softwarecenter.qualcomm.com/api/download/software/qualcomm_neural_processing_sdk/v2.22.6.240515.zip" target="_blank" rel="noopener noreferrer">此处</a>直接下载。下载后，解压软件包。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>上述链接提供的 SDK 版本每季度更新一次。</p>
</blockquote></div></div>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">unzip 2.22.6.240515.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd qairt/2.22.6.240515/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export SNPE_ROOT=`pwd`</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="qualcomm-package-manager-1">Qualcomm Package Manager<a href="#qualcomm-package-manager-1" class="hash-link" aria-label="Direct link to Qualcomm Package Manager" title="Direct link to Qualcomm Package Manager">​</a></h5>
<p>AI Engine Direct SDK 也可以通过 Qualcomm Package Manager (QPM) 下载。本部分演示如何使用 QPM 下载 AI Engine Direct SDK。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>为使用 Qualcomm Package Manager 下载 Qualcomm AI Engine Direct，须确保用户已经注册了 Qualcomm ID。如果用户没有 Qualcomm ID，系统将提示其注册。然后，按照下方说明下载并安装 SDK。</p>
</blockquote></div></div>
<ol>
<li>前往 <a href="https://qpm.qualcomm.com/%23/main/" target="_blank" rel="noopener noreferrer">Qualcomm Package Manager </a>并使用 Qualcomm ID 登录。</li>
</ol>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-181-c9a166431a80be5778aad0eab55b04ac.jpg" width="1000" height="470" class="img_ev3q"></p>
<ol start="2">
<li>点击 <strong>AI Engine Direct SDK</strong>，导航至下一页。从下拉列表中选择 <strong>Linux </strong>和 <strong>Version 2.22.6.240515</strong>，然后点击 <strong>Download </strong>下载 AI Engine Direct SDK 安装程序。</li>
</ol>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-184-93a624b3b9ea73d6fa70169c7c8f4e98.jpg" width="1000" height="477" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<ul>
<li>
<p>这些步骤基于版本 2.22.6.240515。</p>
</li>
<li>
<p>如果使用 Qualcomm Package Manager 桌面工具，<strong>Extract </strong>将取代下载按钮。点击该按钮会自动安装 QNN SDK。</p>
</li>
</ul>
</blockquote></div></div>
<ol start="3">
<li>
<p>安装使用 QPM CLI 工具下载的安装程序 (.qik)。</p>
<ol>
<li>使用 QPM CLI 登录。</li>
</ol>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qpm-cli --login &lt;username&gt;</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-180-3fc002395ff1dc06b8696d8d9ffc48fb.jpg" width="1000" height="164" class="img_ev3q"></p>
<ol start="2">
<li>激活 SDK 许可证。</li>
</ol>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"> qpm</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">cli </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">license</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">activate qualcomm_ai_engine_direct</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-179-92672613fdb32988fe9b643a71456110.jpg" width="1000" height="66" class="img_ev3q"></p>
<ol start="3">
<li>提取并安装 SDK。</li>
</ol>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qpm</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">cli </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">extract </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">path to downloaded </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">qik </span><span class="token builtin">file</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-204-5702331fa2bd25b30f2590ea65131228.jpg" width="1000" height="269" class="img_ev3q"></p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="设置-qualcomm-ai-engine-direct">设置 Qualcomm AI Engine Direct<a href="#设置-qualcomm-ai-engine-direct" class="hash-link" aria-label="Direct link to 设置 Qualcomm AI Engine Direct" title="Direct link to 设置 Qualcomm AI Engine Direct">​</a></h4>
<p><strong>设置 SDK</strong></p>
<p>如需使用 AI Engine Direct SDK，应安装以下版本的 Ubuntu 操作系统和 Python：</p>
<ul>
<li>
<p> Ubuntu 22.04</p>
</li>
<li>
<p> Python 3.10</p>
</li>
</ul>
<p>有关详细的设置说明，可参见 <a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/setup.html" target="_blank" rel="noopener noreferrer">AI Engine Direct SDK 设置</a>。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p><em>${QNN_SDK_ROOT} </em>指向已安装 AI Engine Direct SDK 的根路径。</p>
<ul>
<li>
<p>如果通过直接下载方式下载，须将 <em>${QNN_SDK_ROOT} </em>设置为下载并解压 SDK 的根目录。</p>
</li>
<li>
<p>如果通过 QPM 安装，根目录路径为 <em>/opt/qcom/aistack/qairt/&lt;version&gt;</em>。本示例使用的是 2.22.6.240515 版本，因此路径为：<em>/opt/qcom/ aistack/qairt/2.22.6.240515</em>。</p>
</li>
</ul>
</blockquote></div></div>
<p>SDK 设置步骤如下:</p>
<ul>
<li>
<p>安装 <a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/setup.html#linux-platform-dependencies" target="_blank" rel="noopener noreferrer">Linux 平台依赖项</a></p>
</li>
<li>
<p>使用 <code>${QNN_SDK_ROOT}/bin/check-python-dependency</code> 安装 Python 依赖项</p>
</li>
<li>
<p>使用 <code>sudo bash ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh</code> 安装 Ubuntu 软件包</p>
</li>
<li>
<p>安装模型所需的 ML 框架（PyTorch、TensorFlow、ONNX）。有关支持的框架版本，可参见下表。</p>
</li>
</ul>
<table><thead><tr><th><strong>框架</strong></th><th><strong>版本</strong></th></tr></thead><tbody><tr><td>TensorFlow</td><td>v2.10.1</td></tr><tr><td>TFLite</td><td>v2.3.0</td></tr><tr><td>PyTorch</td><td>v1.13.1</td></tr><tr><td>ONNX</td><td>v1.12.0</td></tr></tbody></table>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Install tensorflow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install tensorflow==2.10.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Install tflite</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install tflite==2.3.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Install PyTorch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install torch==1.13.1+cpu torchvision==0.14.1+cpu torchaudio==0.13.1 --</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">extra-index-url https://download.pytorch.org/whl/cpu</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Install Onnx</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install onnx==1.12.0 onnxruntime==1.17.1 onnxsim==0.4.36</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="配置环境">配置环境<a href="#配置环境" class="hash-link" aria-label="Direct link to 配置环境" title="Direct link to 配置环境">​</a></h5>
<p>用 <code>source</code> 命令执行 QNN SDK 提供的环境配置脚本，确保所有工作流程中必需的工具和库均已在 $PATH 中。</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">source ${QNN_SDK_ROOT}/bin/envsetup.sh</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="qnn-模型移植">QNN 模型移植<a href="#qnn-模型移植" class="hash-link" aria-label="Direct link to QNN 模型移植" title="Direct link to QNN 模型移植">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="模型转换与量化">模型转换与量化<a href="#模型转换与量化" class="hash-link" aria-label="Direct link to 模型转换与量化" title="Direct link to 模型转换与量化">​</a></h5>
<a id="modelconvquant"></a>
<p>将源自 PyTorch、Onnx、TensorFlow 或 TFLite 的 FP32 预训练模型输入到 QNN 转换工具 qnn-&lt;framework&gt;-converter 中，转换为 C++ 格式的可读的 high-level QNN 图。</p>
<p>如果在 HTP 加速器上加速模型，则必须对模型进行量化。模型量化可以与模型转换在同一步骤中完成。在执行此量化步骤时，必须提供校准数据集才能执行静态量化。</p>
<p>如需在转换时同时实现量化，可使用 --input_list INPUT_LIST 选项进行静态量化。有关详细信息，可参见<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/quantization.html" target="_blank" rel="noopener noreferrer">量化支持</a>文档。</p>
<p>以下示例使用的是从 <a href="https://github.com/onnx/models/blob/main/Computer_Vision/inception_v3_Opset16_timm/inception_v3_Opset16.onnx" target="_blank" rel="noopener noreferrer">ONNX Model Zoo </a>下载的 ONNX 模型 (<em>inception_v3_opset16.onnx</em>)。将模型</p>
<p>inception_v3.onnx 下载到工作区中。在本示例中，我们将模型下载到 <em>~/models </em>目录。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="仅模型转换对于-cpu-后端">仅模型转换（对于 CPU 后端）<a href="#仅模型转换对于-cpu-后端" class="hash-link" aria-label="Direct link to 仅模型转换（对于 CPU 后端）" title="Direct link to 仅模型转换（对于 CPU 后端）">​</a></h4>
<p>为转换模型到 x86/Arm CPU 上运行，可运行以下命令，生成<em> inception_v3.cpp</em> 和 <em>inception_v3.bin</em> 文件。</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python ${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-onnx-converter --</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_network ~/models/inception_v3.onnx --output_path ~/models/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3.cpp --input_dim &#x27;x&#x27; 1,3,299,299</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-205-147c08bdd12d49e6aee0477480e1bfe8.jpg" width="1000" height="620" class="img_ev3q"></p>
<p><em>inception_v3.cpp</em> 文件存储转换后模型的 high-level 图。<em>inception_v3.bin</em> 文件存储模型的权重/偏置。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="模型转换和量化对于-htp-后端">模型转换和量化（对于 HTP 后端）<a href="#模型转换和量化对于-htp-后端" class="hash-link" aria-label="Direct link to 模型转换和量化（对于 HTP 后端）" title="Direct link to 模型转换和量化（对于 HTP 后端）">​</a></h4>
<p>如需在 HTP 上运行模型，应对模型进行量化。对于 AI Engine Direct (QNN) SDK 中的量化，从训练数据集中抽取 50 到 200 张图像构成代表性数据集，提供给 QNN 转换工具作为校准数据集。校准数据集中的图像已经过预处理（调整大小、归一化等）并以 NumPy 数组形式保存在 .raw 格式文件中。这些输入的 .raw 文件的大小必须与模型的输入大小相匹配。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>使用 Netron 模型可视化工具来识别模型的输入/输出层的维度。</p>
</blockquote></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-201-28581ca569abd55a5bd35a1e6f4c4941.jpg" width="1000" height="435" class="img_ev3q"></p>
<p>为进行演示，我们可以使用随机输入文件来评估量化过程。可以使用下方所示的 Python 脚本生成 inception_v3.onnx 模型的输入文件。将脚本 <em>generate_random_input.py</em> 保存在 <em>~/models</em> 中，并通过以下方式运行该脚本：<em>python ~/models/ generate_random_input.py</em>。</p>
<p>以下 Python 代码可创建一个 input_list，以包含用于量化模型的校准数据集。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_path_list </span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BASE_PATH </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/tmp/RandomInputsForInceptionV3&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">not</span><span class="token plain"> os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">path</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">exists</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">BASE_PATH</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mkdir</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">BASE_PATH</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># generate 10 random inputs and save as raw</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_IMAGES </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#binary files</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> img </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">NUM_IMAGES</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    filename </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;input_{}.raw&quot;</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">format</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">img</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    randomTensor </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">299</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">299</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">astype</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    filename </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">path</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">join</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">BASE_PATH</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> filename</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    randomTensor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">tofile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">filename</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    input_path_list</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">filename</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#for saving as input_list text</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">with</span><span class="token plain"> </span><span class="token builtin">open</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;~/models/input_list.txt&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;w&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> f</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> path </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> input_path_list</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        f</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">write</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">path</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        f</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">write</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;\n&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>现在，我们可以运行以下命令，对模型进行转换和量化。默认情况下，对模型进行量化时采用 INT8 位宽。开发人员可以使用 [--act_bw 16] 和/或 [--weight_bw 16]，指定 INT16 位宽进行量化。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-onnx-converter --input_network ~/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">models/inception_v3.onnx --output_path ~/models/inception_v3_quantized.cpp --</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_list ~/models/input_list.txt --input_dim &quot;x&quot; 1,3,299,299</span><br></span></code></pre></div></div>
<p>这会在 <em>~/models</em> 目录中生成 <em>inception_v3_quantized.cpp</em> 和 <em>inception_v3_quantized.bin</em> 文件。</p>
<p>可参见 <a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tools.html" target="_blank" rel="noopener noreferrer">qnn-&lt;framework&gt;-converter </a>文档或运行 <code>qnn-&lt;framework&gt;-converter --help</code>， 查看量化所有可用的定制选项，包括量化模式、优化等。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="模型编译">模型编译<a href="#模型编译" class="hash-link" aria-label="Direct link to 模型编译" title="Direct link to 模型编译">​</a></h4>
<a id="modelcompile"></a>
<p>转换/量化步骤完成后，使用 qnn-model-lib-generator 将生成的 C++ 图编译为 Linux 共享对象 (.so)，使应用程序能够动态加载模型，从而执行推理。</p>
<p>对于 x86 架构，可使用 Clang 编译工具链将 C++ 图编译为 .so 库。对于 RUBIK Pi 3 等 Linux 嵌入式设备，必须使用适当的编译工具链 (aarch64-oe-linux-gcc11.2)。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="编译在-x86-上运行的模型">编译在 x86 上运行的模型<a href="#编译在-x86-上运行的模型" class="hash-link" aria-label="Direct link to 编译在 x86 上运行的模型" title="Direct link to 编译在 x86 上运行的模型">​</a></h4>
<p>使用以下命令生成一个可在基于 x86 架构的 Linux 机器上运行的 Linux 共享对象模型。该命令会使用 Clang-14 编译工具链生成 inception_v3.so，将 C++ 图编译为与 x86 主机兼容的 QNN 模型 .so 库。</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator -c ~/models/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3.cpp -b ~/models/inception_v3.bin -o ~/models/libs/ -t x86_64-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">linux-clang</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-202-1f44f64db964aad2f6145559d0453844.jpg" width="1000" height="597" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="编译在目标设备上运行的模型">编译在目标设备上运行的模型<a href="#编译在目标设备上运行的模型" class="hash-link" aria-label="Direct link to 编译在目标设备上运行的模型" title="Direct link to 编译在目标设备上运行的模型">​</a></h4>
<p>在编译用于在设备上执行的模型（aarch64 架构）时，务必要使用适合的交叉编译工具链，确保编译获得的 Linux 共享对象 (.so) 与设备操作系统兼容。</p>
<p>以下步骤用于安装将模型 cpp 文件编译为 .so 库所需的交叉编译工具链。</p>
<p>将以下更改应用到平台 eSDK，以确保 QNN 工具能够与 eSDK 一起正常工作。以下说明使用 ESDK_ROOT 环境变量，该变量设置为平台 eSDK 安装路径。</p>
<ol>
<li>将<em> $ESDK_ROOT/tmp/sysroots/qcm6490</em> 软链接到 <em>$ESDK_ROOT/tmp/sysroots/armv8a-oe-linux</em>。</li>
</ol>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ln -s $ESDK_ROOT/tmp/sysroots/qcm6490 $ESDK_ROOT/tmp/sysroots/armv8a-oe-linux</span><br></span></code></pre></div></div>
<ol start="2">
<li>将<em><strong> </strong>$ESDK_ROOT/tmp/sysroots/x86_64</em> 软链接到<em> $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux。</em></li>
</ol>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ln -s $ESDK_ROOT/tmp/sysroots/x86_64 $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux</span><br></span></code></pre></div></div>
<ul>
<li>将<strong> </strong><em>$ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux/usr/bin/aarch64-qcom-inux</em> 软链接到 <em>$ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux/usr/bin/aarch64-oe-linux。</em></li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ln -s $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux/usr/bin/aarch64-qcom-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">linux $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux/usr/bin/aarch64-oe-linux</span><br></span></code></pre></div></div>
<ol start="4">
<li>将交叉编译器 the cross-compiler $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-</li>
</ol>
<p>linux/usr/bin/aarch64-oe-linux 创建软链接，具体如下：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ln -s $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux/usr/bin/aarch64-oe-linux/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">aarch64-qcom-linux-g++ $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux/usr/bin/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">aarch64-oe-linux/aarch64-oe-linux-g++</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ln -s $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux/usr/bin/aarch64-oe-linux/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">aarch64-qcom-linux-objdump $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">linux/usr/bin/aarch64-oe-linux/aarch64-oe-linux-objdump</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ln -s $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-linux/usr/bin/aarch64-oe-linux/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">aarch64-qcom-linux-objcopy $ESDK_ROOT/tmp/sysroots/x86_64-qtisdk-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">linux/usr/bin/aarch64-oe-linux/aarch64-oe-linux-objcopy</span><br></span></code></pre></div></div>
<ol start="5">
<li>导出以下环境变量，将 QNN 指向交叉编译器：</li>
</ol>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export QNN_AARCH64_LINUX_OE_GCC_112=$ESDK_ROOT/tmp/</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="编译在-arm-cpu-上运行的模型">编译在 Arm CPU 上运行的模型<a href="#编译在-arm-cpu-上运行的模型" class="hash-link" aria-label="Direct link to 编译在 Arm CPU 上运行的模型" title="Direct link to 编译在 Arm CPU 上运行的模型">​</a></h4>
<p>安装交叉编译器后，使用以下命令在 <em>~/model/libs/aarch64-oe-linux-gcc11.2 </em>路径生成 <em>libinception_v3.so</em>。我们可以通过命令行参数将此位置提供给 qnn-model-lib-generator 工具。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>此处使用的编译工具链是：~/model/libs/aarch64-oe-linux-gcc11.2</p>
</blockquote></div></div>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator -c ~/models/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3.cpp -b ~/models/inception_v3.bin -o ~/models/libs -t aarch64-oe-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">linux-gcc11.2</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="编译在-htp-上运行的模型">编译在 HTP 上运行的模型<a href="#编译在-htp-上运行的模型" class="hash-link" aria-label="Direct link to 编译在 HTP 上运行的模型" title="Direct link to 编译在 HTP 上运行的模型">​</a></h4>
<p>为在 HTP 上运行模型，使用以下命令在 <em>~/models/libs/aarch64-oe-linux-gcc11.2 </em>生成 <em>libinception_v3_quantized.so</em>。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>此处使用的编译工具链是：aarch64-oe-linux-gcc11.2.</p>
</blockquote></div></div>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator -c ~/models/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inception_v3_quantized.cpp -b ~/models/inception_v3_quantized.bin -o ~/models/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">libs/ -t aarch64-oe-linux-gcc11.2</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="部署模型">部署模型<a href="#部署模型" class="hash-link" aria-label="Direct link to 部署模型" title="Direct link to 部署模型">​</a></h4>
<a id="deploymodel"></a>
<p>模型 .so（量化或非量化）可以通过支持 QNN 的应用程序（使用 QNN C/C++ API 编写的应用程序）进行部署。QNN 提供 API 来动态加载模型 .so 并使用所选后端在硬件上运行该模型。</p>
<p>QNN 提供了一个预编译工具 (<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tools.html#qnn-net-run" target="_blank" rel="noopener noreferrer">qnn-net-run</a>)，该工具可以动态加载此模型 .so 并使用提供的输入在指定后端执行推理。</p>
<p>在 CPU、GPU 或 HTP 上执行时，qnn-net-run 需要使用三个参数：</p>
<ul>
<li>
<p><strong>模型文件 - </strong>由 qnn-model-lib-generator 生成的 .so 文件</p>
</li>
<li>
<p><strong>后端文件 </strong>- 目标后端所需的 .so 文件</p>
<ul>
<li>
<p>libQnnCpu.so 适用于 CPU 后端。</p>
</li>
<li>
<p>libQnnGpu.so 适用于 GPU 后端。</p>
</li>
<li>
<p>libQnnHtp.so 适用于 HTP 后端。</p>
</li>
</ul>
</li>
<li>
<p><strong>输入列表 -</strong> 文本文件，与量化时使用的 <em>input_list.txt </em>文件类似，但此列表中的输入原始文件用于推理。在本示例中，为简单起见，将量化时使用的同一输入列表用于推理。</p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="在-x86-后端运行-qnn-模型">在 x86 后端运行 QNN 模型<a href="#在-x86-后端运行-qnn-模型" class="hash-link" aria-label="Direct link to 在 x86 后端运行 QNN 模型" title="Direct link to 在 x86 后端运行 QNN 模型">​</a></h5>
<p>如需在 x86 CPU 上运行模型，qnn-net-run 需要使用模型 .so、后端库 .so 和输入列表来运行推理，从而生成输出。</p>
<p>例如，以下命令会加载 libinception_v3.so 模型并在 x86 CPU 上运行该模型。执行完成后，qnn-net-run 工具会将输出文件写入 <em>~/models/output_x86</em>。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-net-run --model ~/models/libs/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">x86_64-linux-clang/libinception_v3.so --backend ${QNN_SDK_ROOT}/lib/x86_64-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">linux-clang/libQnnCpu.so --input_list input_list.txt --output_dir ~/models/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output_qnn_x86</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="准备-qnn-模型以在设备上运行">准备 QNN 模型以在设备上运行<a href="#准备-qnn-模型以在设备上运行" class="hash-link" aria-label="Direct link to 准备 QNN 模型以在设备上运行" title="Direct link to 准备 QNN 模型以在设备上运行">​</a></h5>
<p>在目标设备上运行模型之前，要确保 QNN 二进制文件和库连同模型和输入文件已推送到目标中。</p>
<p>对于 RUBIK Pi 3，使用路径 <em>${QNN_SDK_ROOT}/bin/aarch64-oe-linux-gcc11.2</em> 和<em>${QNN_SDK_ROOT}/lib/aarch64-oe-linux-gcc11.2. </em>中的文件。</p>
<table><thead><tr><th><strong>文件</strong></th><th><strong>源位置</strong></th></tr></thead><tbody><tr><td>qnn-net-run</td><td>${QNN_SDK_ROOT}/bin/aarch64-oe-linux-gcc11.2</td></tr><tr><td>libQnnHtp.so</td><td>${QNN_SDK_ROOT}/lib/aarch64-oe-linux-gcc11.2/ libQnnHtp.so</td></tr><tr><td>libQnnCpu.so</td><td>${QNN_SDK_ROOT}/lib/aarch64-oe-linux-gcc11.2/ libQnnCpu.so</td></tr><tr><td>libQnnGpu.so</td><td>${QNN_SDK_ROOT}/lib/aarch64-oe-linux-gcc11.2/ libQnnGpu.so</td></tr><tr><td>libQnnHtpPrepare.so</td><td>${QNN_SDK_ROOT}/lib/aarch64-oe-linux-gcc11.2</td></tr><tr><td>libQnnHtpV68Stub.so</td><td>${QNN_SDK_ROOT}/lib/aarch64-oe-linux-gcc11.2</td></tr><tr><td>libinception_v3_quantized.so</td><td>~/models/libs/aarch64-oe-linux-gcc11.2</td></tr><tr><td>libinception_v3.so</td><td>~/models/libs/aarch64-oe-linux-gcc11.2</td></tr><tr><td>libQnnHtpV68Skel.so</td><td>${QNN_SDK_ROOT}/lib/hexagon-v68/unsigned</td></tr><tr><td>libqnnhtpv68.cat</td><td>${QNN_SDK_ROOT}/lib/hexagon-v68/unsigned</td></tr><tr><td>libQnnSaver.so</td><td>${QNN_SDK_ROOT}/lib/hexagon-v68/unsigned</td></tr><tr><td>libQnnSystem.so</td><td>${QNN_SDK_ROOT}/lib/hexagon-v68/unsigned</td></tr></tbody></table>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">scp ${QNN_SDK_ROOT}/bin/aarch64-oe-linux-gcc11.2/qnn-* root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ${QNN_SDK_ROOT}/lib/aarch64-oe-linux-gcc11.2/libQnn*.so root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ${QNN_SDK_ROOT}/lib/hexagon-v68/unsigned/* root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ~/models/libs/aarch64-oe-linux-gcc11.2/* root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp ~/models/input_list.txt root@[ip-addr]:/opt/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp /tmp/RandomInputsForInceptionV3 root@[ip-addr]:/tmp/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ssh root@[ip-addr]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd /opt/</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="在-arm-cpu-上运行-qnn-模型">在 Arm CPU 上运行 QNN 模型<a href="#在-arm-cpu-上运行-qnn-模型" class="hash-link" aria-label="Direct link to 在 Arm CPU 上运行 QNN 模型" title="Direct link to 在 Arm CPU 上运行 QNN 模型">​</a></h5>
<p>在基于 Arm 架构的 CPU 上执行模型时，qnn-net-run 需要使用模型 .so（对于 RUBIK Pi 3 目标设备，.so 必须使用 aarch64-oe-linux-gcc11.2 工具链进行交叉编译）、后端 .so 库和输入列表来运行推理，从而生成输出。</p>
<p>例如，以下命令会将输出文件写入 <em>/opt/output_cpu</em>。</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export LD_LIBRARY_PATH=/opt/:$LD_LIBRARY_PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export PATH=/opt:$PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">qnn-net-run --model libinception_v3.so --backend libQnnCpu.so --input_list</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_list.txt --output_dir output_cpu</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="在-gpu-上运行-qnn-模型">在 GPU 上运行 QNN 模型<a href="#在-gpu-上运行-qnn-模型" class="hash-link" aria-label="Direct link to 在 GPU 上运行 QNN 模型" title="Direct link to 在 GPU 上运行 QNN 模型">​</a></h5>
<p>在 Adreno GPU 上执行模型时，qnn-net-run 需要使用模型 .so、后端 .so 库 (libQnnGpu.so) 和输入列表来运行推理，从而生成输出。</p>
<p>例如，以下命令会将输出文件写入<em>/opt/output_gpu</em>。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export LD_LIBRARY_PATH</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token operator" style="color:#393A34">/</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">$LD_LIBRARY_PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export PATH</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">$PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">qnn</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">net</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">run </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">model libinception_v3</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">so </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">backend libQnnGpu</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">so </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">input_list</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_list</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">txt </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">output_dir output_gpu</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="在-htp-后端运行-qnn-模型">在 HTP 后端运行 QNN 模型<a href="#在-htp-后端运行-qnn-模型" class="hash-link" aria-label="Direct link to 在 HTP 后端运行 QNN 模型" title="Direct link to 在 HTP 后端运行 QNN 模型">​</a></h5>
<p>如需在 HTP 后端运行模型，可使用 libquantized_inception_v3.so 库和 libQnnHtp.so 后端库，并将输出保存在 <em>output_htp </em>目录中。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export LD_LIBRARY_PATH=/opt/:$LD_LIBRARY_PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export PATH=/opt:$PATH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export ADSP_LIBRARY_PATH=&quot;/opt/;/usr/lib/rfsa/adsp;/dsp&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">qnn-net-run --model libinception_v3_quantized.so --backend libQnnHtp.so --</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_list input_list.txt --output_dir output_htp</span><br></span></code></pre></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="对输出进行验证-1">对输出进行验证<a href="#对输出进行验证-1" class="hash-link" aria-label="Direct link to 对输出进行验证" title="Direct link to 对输出进行验证">​</a></h5>
<p>对于每个送入 qnn-net-run（通过 input_list 文件）的输入原始文件，都会生成一个输出文件夹，其中存储输出原始文件，文件大小将与模型的输出层相匹配，如下图所示。（<a href="https://netron.app/" target="_blank" rel="noopener noreferrer">Netron </a>是一种模型可视化工具）</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-203-43e8afb2be30336952ef565e898e0e9a.jpg" width="1000" height="520" class="img_ev3q"></p>
<p>对于本示例 inception_v3，输出原始文件是一个二进制文件，其中存储 1000 个分类类别的概率。</p>
<p>我们使用 Python 脚本将文件读取为 NumPy 数组，以执行后处理步骤，并对输出进行验证。下方示例用于检查 HTP 预测结果是否与 CPU 预测结果相同。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ssh root@</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">ip</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">addr</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd </span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">r </span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">output_cpu user@host</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">ip</span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">path</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scp </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">r </span><span class="token operator" style="color:#393A34">/</span><span class="token plain">opt</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">output_htp user@host</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">ip</span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">path</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><blockquote>
<p>在下方 Python 脚本中，使用以上命令中的 <em>&lt;path&gt;</em>。</p>
</blockquote></div></div>
<p>将 qnn-net-run 工具的输出从设备复制到主机后，我们即可创建一个简单的 Python 脚本，用于加载在 CPU 和 HTP 上执行的输出，并使用 NumPy 对它们进行比较。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">#python postprocessing script (compare.py)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">htp_output_file_path </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;&lt;path&gt;/output_htp/Result_1/875.raw&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cpu_output_file_path </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;&lt;path&gt;/output_cpu/Result_1/875.raw&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">htp_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">fromfile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">htp_output_file_path</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">htp_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> htp_output</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">reshape</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cpu_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">fromfile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cpu_output_file_path</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cpu_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cpu_output</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">reshape</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cls_id_htp </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">argmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">htp_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cls_id_cpu </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">argmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cpu_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Let&#x27;s compare CPU output vs HTP output</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;CPU prediction {} \n HTP prediction {}&quot;</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">format</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cls_id_cpu</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> cls_id_htp</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>输出</p>
<p><img decoding="async" loading="lazy" src="/linux120.github.io/assets/images/image-200-c1d99319b42f24e48a25626fdbc13192.jpg" width="1000" height="184" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="使用-qnn-api-部署模型">使用 QNN API 部署模型<a href="#使用-qnn-api-部署模型" class="hash-link" aria-label="Direct link to 使用 QNN API 部署模型" title="Direct link to 使用 QNN API 部署模型">​</a></h5>
<p>Qualcomm AI Engine Direct SDK 提供 C/C++ API，用于创建/开发应用程序，以加载已编译 .so 模型并在所选后端（CPU、GPU 或 HTP）上加速执行。<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/sample_app.html" target="_blank" rel="noopener noreferrer">此处</a>是一个使用 SNPE C/C++ API 开发的用于执行模型的示例程序。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/5.ai-developer-workflow.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/linux120.github.io/docs/qualcomm-ai-hub"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Qualcomm AI Hub</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/linux120.github.io/docs/qualcomm-im-sdk"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Qualcomm IM SDK</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#概述" class="table-of-contents__link toc-highlight">概述</a><ul><li><a href="#ai-硬件" class="table-of-contents__link toc-highlight">AI 硬件</a></li><li><a href="#ai-软件" class="table-of-contents__link toc-highlight">AI 软件</a></li></ul></li><li><a href="#编译并优化模型" class="table-of-contents__link toc-highlight">编译并优化模型</a><ul><li><a href="#ai-hub" class="table-of-contents__link toc-highlight">AI Hub</a><ul><li><a href="#环境配置" class="table-of-contents__link toc-highlight">环境配置</a></li><li><a href="#ai-hub-工作流程" class="table-of-contents__link toc-highlight">AI Hub 工作流程</a></li></ul></li><li><a href="#tflite" class="table-of-contents__link toc-highlight">TFLite</a></li><li><a href="#qualcomm-神经网络处理引擎" class="table-of-contents__link toc-highlight">Qualcomm 神经网络处理引擎</a><ul><li><a href="#安装-qualcomm-神经网络处理引擎-sdk" class="table-of-contents__link toc-highlight">安装 Qualcomm 神经网络处理引擎 SDK</a></li><li><a href="#设置-qualcomm-神经网络处理-sdk" class="table-of-contents__link toc-highlight">设置 Qualcomm 神经网络处理 SDK</a></li><li><a href="#使用-snpe-移植模型" class="table-of-contents__link toc-highlight">使用 SNPE 移植模型</a></li></ul></li><li><a href="#qualcomm-ai-engine-direct-sdk" class="table-of-contents__link toc-highlight">Qualcomm AI Engine Direct SDK</a><ul><li><a href="#安装-ai-engine-direct" class="table-of-contents__link toc-highlight">安装 AI Engine Direct</a></li><li><a href="#设置-qualcomm-ai-engine-direct" class="table-of-contents__link toc-highlight">设置 Qualcomm AI Engine Direct</a></li><li><a href="#qnn-模型移植" class="table-of-contents__link toc-highlight">QNN 模型移植</a></li><li><a href="#仅模型转换对于-cpu-后端" class="table-of-contents__link toc-highlight">仅模型转换（对于 CPU 后端）</a></li><li><a href="#模型转换和量化对于-htp-后端" class="table-of-contents__link toc-highlight">模型转换和量化（对于 HTP 后端）</a></li><li><a href="#模型编译" class="table-of-contents__link toc-highlight">模型编译</a></li><li><a href="#编译在-x86-上运行的模型" class="table-of-contents__link toc-highlight">编译在 x86 上运行的模型</a></li><li><a href="#编译在目标设备上运行的模型" class="table-of-contents__link toc-highlight">编译在目标设备上运行的模型</a></li><li><a href="#编译在-arm-cpu-上运行的模型" class="table-of-contents__link toc-highlight">编译在 Arm CPU 上运行的模型</a></li><li><a href="#编译在-htp-上运行的模型" class="table-of-contents__link toc-highlight">编译在 HTP 上运行的模型</a></li><li><a href="#部署模型" class="table-of-contents__link toc-highlight">部署模型</a></li></ul></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/linux120.github.io/docs/intro">Tutorial</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/linux120.github.io/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>